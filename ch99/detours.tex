%%chapter%% 99
\chapter{Detours}

%\newcommand{\detour}[2]{\textbf{#2}\label{detour:#1}}
\newcommand{\detour}[2]{\subsection{#2}\label{detour:#1}}

\vfill

\detour{def-tangent}{Formal definition of the tangent line}\index{tangent line!formal definition}

Let $(a,b)$ be a point on the graph of the function $x(t)$.
A line $\ell(t)$ through this point is said not to cut through the graph
if there exists some real number $d>0$ such that
$x(t)-\ell(t)$ has the same sign for all $t$ between $a-d$ and $a+d$.
The line is said to be the tangent line at this point if it is the
only line through this point that doesn't cut through the graph.

As an exception,
there are cases in which the function is smooth and well-behaved throughout a certain
region, but has no tangent line according to this definition at one particular point.
For example, the function $x(t)=t^3$ has tangent lines everywhere except at $t=0$,
which is an inflection point (p. \pageref{inflection}). In such cases, we fill in the
``gap tooth'' in the derivative function in the obvious way.


\pagebreak

\detour{polynomial-proof}{Derivatives of polynomials}\index{derivative!of a polynomial}

Some ideas in this proof are due to Michael Livshits.

We want to prove that the derivative of $t^k$ is $kt^{k-1}$. It suffices to
prove that the derivative equals $k$ when evaluated at $t=1$, since we can
then apply the kind of scaling argument\footnote{In the special case of $t=0$,
the scaling argument doesn't work. For even $k$, it can be easily verified that
the derivative at $t=0$ equals zero. For odd $k$, we have to fill in the ``gap tooth''
as mentioned in the preceding section.} used on page \pageref{scaling} to
show that the derivative of $t^2/2$ was $t$. The proposed tangent line at $(1,1)$
has the equation $\ell=k(t-1)+1$, so what we need to prove is that
the polynomial $t^k-[k(t-1)+1]$ is greater than or equal to zero throughout some
region around $t=1$.

Figure \figref{polyn-derivative-proof-secant} shows a typical case. 
The graph of $3(t-1)+1$ lies entirely below the graph of $t^3$ in a large region.
It does pop back up above it at $t=-2$, but that's far away, and the definition of
the tangent line only requires that some region around $(1,1)$ be free of such
crossing points. In fact, a little experimentation shows that these crossings occur
only for odd $k$, and always for $t<0$. This suggests that we ought to aim for
a general proof that there are no crossings for $t \ge 0$.

%%graph%% polyn-derivative-proof-secant func=x**3 format=eps xlo=-3 xhi=2 ylo=-15 yhi=10 with=lines x=t y=x ytic_spacing=5 ; func=3*(x-1)+1 with=lines
\smallfig{polyn-derivative-proof-secant}{The graphs of $t^3$ and $3(t-1)+1$.}

Suppose that such a crossing happens at the point $(t,t^k)$. Then the slope of the
line $\ell(t)$ is $k$, so we must have
\begin{equation*}
  \frac{t^k-1}{t-1} = k \qquad .
\end{equation*}
The left-hand side is the quotient of two polynomials, and if we try the example
of $k=3$, we find that the division comes out with a remainder of zero, and
is in the very simple form $t^2+t+1$, i.e., a polynomial of order $k-1$ whose
coefficients are all equal to 1. We can easily verify that this works for
all $k$, by checking the multiplication
\begin{equation*}
  t^k-1 = (t-1)(t^k+t^{k-1}+\ldots+1) \qquad ,
\end{equation*}
in which all the terms in the expansion of the right-hand side cancel except for $t^k$ and $-1$.
Let's refer to the quotient as $Q(t)=t^k+t^{k-1}+\ldots+1$.
How can we get $Q(t)=k$? Clearly we have a solution for $t=1$, since there are $k$ terms,
each equal to 1. For $t>1$, all the terms except the constant one are greater than 1, so
there can't be any solution. For $0 \le t < 1$, all the terms except the constant
one are positive and less than 1, so again there can't be any solution.
This completes the proof that there are no crossings for $t \ge 0$, which establishes
the desired result.

\detour{sin-rigor}{Details of the proof of the derivative of the sine function}\index{derivative!of the sine}

Some ideas in this proof are due to Jerome Keisler.

On page \pageref{eg:derivative-of-sin}, I computed
\begin{align*}
  \der x &= \sin(t+\der t)-\sin t \qquad , \\
         &= \sin t \: \cos \der t \\
         &\quad + \cos t \: \sin \der t - \sin t \\
         &\approx \cos t \: \der t \qquad .
\end{align*}
Here I'll prove that the error introduced by the small-angle approximations really
is of order $\der t^2$. We have
\begin{equation*}
  \sin(t+\der t) = \sin t + \cos t \der t - E \qquad ,
\end{equation*}
where the error $E$ introduced by the approximations is
\begin{align*}
  E = &\sin t (1-\cos \der t) \\
    + &\cos t (\der t - \sin \der t) \qquad .
\end{align*}

\smallfig{sin-rigor}{Geometrical interpretation of the error term.}

Let the radius of the circle in figure \figref{sin-rigor} be one, so AD
is $\cos \der t$ and CD is $\sin \der t$. The area of the shaded pie slice
is $\der t/2$, and the area of triangle ABC is $\sin\der t/2$, so the
error made in the approximation $\sin\der t\approx\der t$ equals twice
the area of the dish shape formed by line BC and arc BC. Therefore
$\der t-\sin\der t$ is less than the area of rectangle CEBD.
But CEBD has both an infinitesimal width and an infinitesimal height,
so this error is of no more than order $\der t^2$.

For the approximation $\cos\der t\approx 1$, the error (represented
by BD) is $1-\cos\der t=1-\sqrt{1-\sin^2\der t}$, which is less
than $1-\sqrt{1-\der t^2}$, since $\sin \der t<\der t$. Therefore
this error is of order $\der t^2$.

\detour{transcendentals}{The transfer principle applied to functions}

On page \pageref{transcendentals}, I told you not to worry about whether it was legitimate to
apply familiar functions like $x^2$, $\sqrt{x}$, $\sin x$, $\cos x$, and $e^x$ to hyperreal numbers.
But since you're reading this, you're obviously in need of more reassurance.

For some of these functions, the transfer principle straightforwardly guarantees that they work
for hyperreals, have all the familiar properties, and can be computed in the same way. For example,
the following statement is in a suitable form to have the transfer principle applied to it:
\emph{
  For any real number $x$, $x\cdot x \ge0$.
}
Changing ``real'' to ``hyperreal,'' we find out that the square of a hyperreal number is
greater than or equal to zero, just like the square of a real number. Writing it as $x^2$ or
calling it a square is just a matter of notation and terminology.
The same applies to this statement:
\emph{
  For any real number $x\ge 0$, there exists a real number $y$ such that $y^2=x$.
}
Applying the transfer function to it tells us that square roots can be defined for
the hyperreals as well.

There's a problem, however, when we get to functions like $\sin x$ and $e^x$.
If you look up the definition of the sine function in a trigonometry textbook, it will
be defined geometrically, as the ratio of the lengths of two sides of a certain triangle.
The transfer principle doesn't apply to geometry, only to arithmetic. It's not even obvious
intuitively that it makes sense to define a sine function on the hyperreals. In an
application like the differentiation of the sine function on page \pageref{eg:derivative-of-sin},
we only had to take sines of hyperreal numbers that were infinitesimally close to real numbers,
but if the sine is going to be a full-fledged function defined on the hyperreals, then we should
be allowed, for example, to take the sine of an infinite number. What would that mean? If you
take the sine of a number like a million or a billion on your calculator, you just get some
apparently random result between $-1$ and 1. The sine function wiggles back and forth indefinitely
as $x$ gets bigger and bigger, never settling down to any specific limiting value. Apparently
we could have $\sin H=1$ for a particular infinite $H$, and then $\sin (H+\pi/2)=0$, $\sin(H+\pi)=-1$, \ldots

It turns out that the moral equivalent of the  transfer function can indeed be applied to any function on
the reals, yielding a function that is in some sense its natural ``big brother'' on the the hyperreals, but the consequences can be
either disturbing or exhilirating depending on your tastes.\index{transfer principle!applied to functions}
 For example, consider the function $[x]$ that takes
a real number $x$ and rounds it down to the greatest integer that is less than or equal to to $x$, e.g.,
$[3]=3$, and $[\pi]=3$. This function, like any other real function,
can be extended to the hyperreals, and that means that we can define
the \emph{hyperintegers},\index{hyperinteger}
the set of hyperreals that satisfy $[x]=x$. The hyperintegers include the integers as a subset,
but they also include infinite numbers. This is likely to seem magical, or even unreasonable, if we come
at the hyperreals from the axiomatic point of view,
as in this book and Keisler's more detailed treatment in
\emph{Elementary Calculus: An Approach Using Infinitesimals}, \url{http://www.math.wisc.edu/~keisler/calc.html}.
The extension of functions to the hyperreals seems much more natural in an alternative,
constructive approach, which is explained admirably in an online article at \url{http://mathforum.org/dr.math/faq/analysis_hyperreals.html}.

\detour{chain-rule}{Proof of the chain rule}

In the statement of the chain rule on page \pageref{sec:chain-rule}, I followed my usual custom of writing
derivatives as $\der y/\der x$, when actually the derivative is the standard part, $\st(\der y/\der x)$. In more rigorous
notation, the chain rule should be stated like this:
\begin{equation*}
  \st\left(\frac{\der z}{\der x}\right) =   \st\left(\frac{\der z}{\der y}\right) \st\left(\frac{\der y}{\der x}\right) \qquad .
\end{equation*}
The transfer principle allows us to rewrite the left-hand side as $\st[(\der z/\der y)(\der y/\der x)]$, and then
we can get the desired result using the identity $\st(ab)=\st(a)\st(b)$.

\detour{exp}{Derivative of $e^x$}\index{derivative!of the exponential}

All of the reasoning on page \pageref{main:exp} would have applied equally well to any other
exponential function with a different base, such as $2^x$ or $10^x$. Those functions would have
different values of $c$, so if we want to determine the value of $c$ for the base-$e$ case, we
need to bring in the definition of $e$, or of the exponential function $e^x$, somehow.

We can take the definition of $e^x$ to be\label{definition-of-exp}\index{exponential!definition of}
\begin{align*}
  e^x = \lim_{n\rightarrow \infty} \left(1+\frac{x}{n}\right)^n \qquad .
\end{align*}
The idea behind this relation is similar to the idea of compound interest. If the interest rate is 10\%, compounded
annually, then $x=0.1$, and the balance grows by a factor $(1+x)=1.1$ in one year. If, instead, we want to compound the
interest monthly, we can set the monthly interest rate to $0.1/12$, and then the growth of the
balance over a year is $(1+x/12)^{12}=1.1047$, which is slightly larger because the interest from the earlier months
itself accrues interest in the later months. Continuing this limiting process, we find $e^{1.1}=1.1052$.

If $n$ is large, then we have a good approximation to the base-$e$ exponential, so let's differentiate
this finite-$n$ approximation and try to find an approximation to the derivative of $e^x$. The chain rule
tells is that the derivative of $(1+x/n)^n$ is the derivative of the raising-to-the-nth-power function,
multiplied by the derivative of the inside stuff, $\der(1+x/n)/\der x=1/n$. We then have
\begin{align*}
  \frac{\der \left(1+\frac{x}{n}\right)^n}{\der x} &= \left[n\left(1+\frac{x}{n}\right)^{n-1}\right]\cdot \frac{1}{n} \\
            &= \left(1+\frac{x}{n}\right)^{n-1} \qquad .
\end{align*}
But evaluating this at $x=0$ simply gives 1, so at $x=0$, the approximation to the derivative is exactly 1 for all values of
$n$ --- it's not even necessary to imagine going to larger and larger values of $n$. This establishes that $c=1$,
so we have
\begin{equation*}
  \frac{\der e^x}{\der x} = e^x 
\end{equation*}
for all values of $x$.

\detour{fundamental-thm-proof}{Proof of the fundamental theorem of calculus}\index{calculus!fundamental theorem of!proof}\index{fundamental theorem of calculus!proof}

There are three parts to the proof: (1)  Take the equation that states
the fundamental theorem, differentiate both sides with respect to $b$, and show that they're equal.
(2) Show that continuous functions with equal derivatives must be essentially the same function, except for an
additive constant. (3) Show that the constant in question is zero.

1. By the definition of the indefinite integral, the derivative of $x(b)-x(a)$ with respect to $b$ equals
$\xdot(b)$. We have to establish that this equals the following:
\begin{align*}
  \frac{\der}{\der b} \int_a^b \xdot(t) \der t 
    &= \st \frac{1}{\der b}\left[\int_a^{b+\der b}  \xdot(t) \der t - \int_a^b  \xdot(t) \der t\right] \\
    &= \st \frac{1}{\der b}\int_b^{b+\der b}  \xdot(t) \der t \\
    &= \st \frac{1}{\der b}\lim_{H\rightarrow\infty} \sum_{i=0}^H  \xdot(b\:+\:i\:\der b/H) \frac{\der b}{H} \\
    &= \st \lim_{H\rightarrow\infty} \frac{1}{H} \sum_{i=0}^H  \xdot(b\:+\:i\:\der b/H)
\end{align*}
Since $\xdot$ is continuous, all the values of $\xdot$ occurring inside the sum can differ
only infinitesimally from $\xdot(b)$. Therefore the quantity inside the limit differs only infinitesimally from
$\xdot(b)$, and the standard part of its limit must be $\xdot(b)$.\footnote{If you don't want to use infinitesimals,
then you can express the derivative as a limit, and in the final step of the argument use the mean value theorem,
introduced later in the chapter.}

2. Suppose $f$ and $g$ are two continuous functions whose derivatives are equal. Then $d=f-g$ is a continuous function whose derivative is zero.
But the only continuous function with a derivative of zero is a constant, so $f$ and $g$ differ by at most an additive constant.

3. I've established that the derivatives with respect to $b$ of $x(b)-x(a)$ and $\int_a^b \xdot \der t$ are the same, so they differ by
at most an additive constant. But at $b=a$, they're both zero, so the constant must be zero.

\pagebreak

\detour{intermediate-value}{The intermediate value theorem}\index{intermediate value theorem}

On page \pageref{intermediate-value-ref-to-detour} I asserted that the intermediate value theorem was
really more a statement about the (real or hyperreal) number system than about functions. For insight,
consider figure \figref{euclid}, which is a geometrical construction that constitutes the proof of the
very first proposition in Euclid's celebrated \emph{Elements}. The proposition to be proved is that given
a line segment AB, it is possible to construct an equilateral triangle with AB as its base. The proof is
by construction; that is, Euclid doesn't just give a logical argument that convinces us the triangle must
exist, he actually demonstrates how to construct it. First we draw a circle with center
A and radius AB, which his third postulate says we can do. Then we draw another circle with the same radius,
but centered at B. Pick one of the intersections of the circles and call it C. Construct the line segments
AC and BC (postulate 1). Then AC equals AB by the definition of the
circle, and likewise BC equals AB. Euclid also has an axiom that things equal
to the same thing are equal to one another, so it follows that AC equals BC, and therefore the triangle is equilateral.

\fig{euclid}{A proof from Euclid's \emph{Elements}.}

It seems like a model of mathematical rigor, but there's a flaw in the reasoning, which is that he assumes
without justififcation that the circles do have a point in common. To see that this is not as secure an assumption as it seems, consider
the usual Cartesian representation of plane geometry in terms of coordinates $(x,y)$.
Usually we assume that $x$ and $y$ are real numbers. What if we instead do our Cartesian geometry
using rational numbers as coordinates? Euclid's five postulates are all consistent with this.
For example, circles do exist. Let $\zu{A}=(0,0)$ and $\zu{B}=(1,0)$. Then there are infinitely
many pairs of rational numbers in the set that satisfies the definition of the circle centered at A. Examples include $(3/5,4/5)$
and $(-7/25,24/25)$. The circle is also continuous in the sense that if I specify a point on it such
as $(-7/25,24/25)$, and a distance that I'm allowed to make as small as I please, say $10^{-6}$, then other
points exist on the circle within that distance of the given point. However, the intersection
assumed by Euclid's proof doesn't exist. It would lie at $(1/2,\sqrt{3}/2)$, but $\sqrt{3}$ doesn't
exist in the rational number system.

In exactly the same way, we can construct counterexamples to the intermediate value theorem if
the underlying system of numbers doesn't have the same properties as the real numbers. For example,
let $y=x^2$. Then $y$ is a continuous function, on the interval from 0 to 1, but if we take the rational
numbers as our foundation, then there is no $x$ for which $y=1/2$. The solution would be $x=1/\sqrt{2}$, which
doesn't exist in the rational number system. Notice the similarity between this problem and the one in
Euclid's proof. In both cases we have curves that cut one another without having an intersection.
In the present example, the curves are the graphs of the functions $y=x^2$ and $y=1/2$.

The interpretation is that the real numbers are in some sense more densely packed than the rationals,
and with two thousand years worth of hindsight, we can see that Euclid should have included a
sixth postulate that expressed this density property. One possible way of stating such a postulate
is the following. Let L be a ray, and O its endpoint. We think of O as the
origin of the positive number line.
Let P and Q be sets of points on L such that every point in P is closer to O than every point in Q.
Then there exists some point Z on L such that Z lies at least as far from O as every point in P,
but no farther than any point in Q. Technically this property is known as \emph{completeness}.\index{completeness}
As an example, let $\zu{P}=\{x|x^2<2\}$ and $\zu{Q}=\{x|x^2 \ge 2\}$. Then the point Z would have to be $\sqrt{2}$,
which shows that the rationals are not complete. The reals are complete, and the completeness
axiom can serve as one of the fundamental axioms of the real numbers.

Note that the axiom
refers to \emph{sets} P and Q, and says that a certain fact is true for any choice of those sets;
it therefore isn't the type of proposition that is covered by the transfer principle, and in fact
it fails for the hyperreals, as we can see if P is the set of all infinitesimals and Q the
positive real numbers.

Here is a skeletal proof of the intermediate value theorem, in which I'll make some simplifying
assumptions and leave out some cases. We want to prove that if $y$ is a continuous real-valued function on the real interval from $a$ to $b$,
and if $y$ takes on values $y_1$ and $y_2$ at certain points within this interval, then for any $y_3$ between $y_1$ and
$y_2$, there is some real $x$ in the interval for which $y(x)=y_3$.
I'll assume the case in which $x_1<x_2$ and $y_1<y_2$.
Define sets of real numbers $\zu{P}=\{x|y \le y_3\}$, and let $\zu{Q}=\{x|y \ge y_3\}$.
For simplicity, I'll assume that every member of P is less than or equal to
every member of Q, which happens, for example, if the function $y(x)$ is always increasing
on the interval $[a,b]$. If P and Q intersect, then the theorem holds.
Suppose instead that P and Q do not intersect.
Using the completeness axiom, there exists some real $x$ which is greater than or equal
to every element of P and less than or equal to every element of Q.
Suppose $x$ belongs to P.
Then the following
statement is in the right form for the transfer principle to apply to it: for any number
$x'>x$, $y(x')>y_3$. We can conclude that the statement is also true for the hyperreals,
so that if $\der x$ is a positive infinitesimal and $x'=x+\der x$, we have $y(x)<y_3$, but $y(x+\der x)>y_3$.
Then by continuity, $y(x)-y(x+\der x)$ is infinitesimal. But $y(x)<y_3$ and
$y(x+\der x)>y_3$, so the standard part of $y(x)$ must equal $y_3$. By assumption $y$ takes on real values
for real arguments, so $y(x)=y_3$. The same reasoning applies if $x$ belongs to Q, and since
$x$ must belong either to P or to Q, the result is proved.

For an alternative proof of the
intermediate value theorem by an entirely different technique, see Keisler's
\emph{Elementary Calculus: An Approach Using Infinitesimals}, p. 162, available online at
\verb@http://www.math.wisc.edu/~keisler/calc.html@.

As a side issue, we could ask whether there is anything like the intermediate value theorem
that can be applied to functions on the hyperreals. Our definition of continuity on page \pageref{def-continuity}
explicitly states that it only applies to real functions.
Even if we could apply the definition to a function on the hyperreals, the proof given above would fail, since the hyperreals lack the completeness
property. As a counterexample, let $\epsilon$ be some positive infinitesimal, and define
a function $y$ such that $y=-\epsilon$ when $\zu{st}(x) \le 0$ and $y=\epsilon$ everywhere else.
If we insist on applying the definition of continuity to this function, it appears to be continuous,
so it violates the intermediate value theorem. Note, however, that the way this function is defined
is different from the way we usually define functions on the hyperreals. Usually we define a function
on the reals, say $y=x^2$, in language to which the transfer principle applies, and then we use the
transfer principle to reason about the function's analog on the hyperreals. For instance, the function
$y=x^2$ has the property that $y \ge 0$ everywhere, and the transfer principle guarantees that that's
also true if we take $y=x^2$ as the definition of a function on the hyperreals. For functions defined
in this way, the intermediate value theorem makes a statement that the transfer principle applies
to, and it is therefore true for the hyperreal version of the function as well.

\detour{extreme-value}{Proof of the extreme value theorem}\index{extreme value theorem!proof}

The extreme value theorem was stated on page \pageref{extreme-value-theorem}. Before we can prove
it, we need to establish some preliminaries, which turn out to be interesting for their own sake.

Definition: Let $C$ be a subset of the real numbers whose definition can be expressed
in the type of language to which the transfer principle applies. Then $C$ is \emph{compact}\index{compact set}
if for every hyperreal number $x$ satisfying the definition of $C$, the standard part of $x$ exists
and is a member of $C$.

To understand the content of this definition, we need to look at the two ways in which a set could
fail to satisfy it. 

First, suppose $U$ is defined by $x \ge 0$. Then there are positive infinite
hyperreal numbers that satisfy the definition, and their standard part is not defined, so $U$ is not
compact. The reason $U$ is not compact is that it is unbounded.

Second, let $V$ be defined by $0 \le x < 1$. Then if $dx$ is a positive infinitesimal,
$1-dx$ satisfies the definition of $V$, but its standard part is 1, which is not in $V$, so $V$
is not compact. The set $V$ has boundary points at 0 and 1, and the reason it is not compact
is that it doesn't contain its right-hand boundary point. A boundary point\index{boundary point}
is a real number which is infinitesimally close to some points inside the set, and also to some
other points that are on the outside.

We therefore arrive at the following alternative characterization of the notion of a compact
set, whose proof is straightforward.

Theorem: A set is compact if and only if it is bounded and contains all of its boundary points.

Intuitively, the reason compact sets are interesting is that if you're standing inside a compact
set and start taking steps in a certain direction, without ever turning around, you're guaranteed to
approach some point in the set as a limit. (You might step over some gaps that aren't included in the set.)
If the set was unbounded, you could just walk forever at a constant speed.
If the set didn't contain its boundary point, then you could asymptotically approach the boundary, but
the goal you were approaching wouldn't be a member of the set. In other words, every increasing
sequence of numbers within a compact set approaches a limit within that set, and similarly for every
decreasing sequence.

The following theorem turns out to be the most difficult part of the discussion.

Theorem: A compact set contains its maximum and minimum.\\
Proof: Let $C$ be a compact set. We know it's bounded, so let $M$ be the set of all real numbers
that are greater than any member of $C$. By the completeness property of the real
numbers, there is some real number $x$ between $C$ and $M$. Let $^{*}C$ be the set of hyperreal numbers that satisfies the same
definition that $C$ does.

Every real $x'$ greater than $x$ fails
to satisfy the condition that defines $C$, and by the transfer principle the same must be true if $x'$ is any hyperreal,
so if $dx$ is a positive infinitesimal, $x+dx$ must be outside of $^{*}C$. 

But now consider $x-dx$.
The following statement holds for the reals: there is no number $x'<x$ that is greater than every
member of $C$. By the transfer principle, we find that there is some hyperreal number $q$ in $^{*}C$
that is greater than $x-dx$. But the standard part of $q$ must equal $x$, for otherwise $\zu{st} q$ would
be a member of $C$ that was greater than $x$.
Therefore $x$ is a boundary point of $C$, and since
$C$ is compact, $x$ is a member of $C$. We conclude $C$ contains its maximum. A similar argument shows that
$C$ contains its minimum, so the theorem is proved.

There were two subtle things about this proof. The first was that we ended up constructing the
set of hyperreals $^{*}C$, which was the hyperreal ``big brother'' of the real set $C$. This
is exactly the sort of thing that the transfer principle does \emph{not} guarantee we can do.
However, if you look back through the proof, you can see that $^{*}C$ is used only as a notational
convenience. Rather than talking about whether a certain number was a member of $^{*}C$, we could
have referred, more cumbersomely, to whether or not it satisfied the condition that had originally
been used to define $C$. The price we paid for this was a slight loss of generality. There are so
many different sets of real numbers that they can't possibly all have explicit definitions that can
be written down on a piece of paper. However, there is very little reason to be interested in
studying the properties of a set that we were never able to define in the first place. The
other subtlety was that we had to construct the auxiliary
point $x-dx$, but there was not much we could actually say about $x-dx$ itself. In particular, it
might or might not have been a member of $C$. For example, if $C$ is defined by the condition $x=0$, then
$^{*}C$ likewise contains only the single element 0, and $x-dx$ is not a member of $^{*}C$.
But if $C$ is defined by $0 \le x \le 1$, then $x-dx$ is a member of $^{*}C$.

The original goal was to prove the extreme value theorem, which is a statement about continuous functions, but so far we haven't said anything about functions.

Lemma: Let $f$ be a real function defined on a set of points $C$. Let $D$ be the image of $C$, i.e.,
the set of all values $f(x)$ that occur for some $x$ in $C$. Then if $f$ is continous and $C$ is
compact, $D$ is compact as well. In other words, continuous functions take compact sets to compact sets.\\
Proof: Let $y=f(x)$ be any hyperreal output corresponding to a hyperreal input $x$ in $^{*}C$.
We need to prove that the standard part of $y$ exists, and is a member of $D$. Since $C$ is compact,
the standard part of $x$ exists and is a member of $C$. But then by continuity $y$ differs only
infinitesimally from $f(\zu{st} x)$, which is real, so $\zu{st} y=f(\zu{st} x)$ is defined and is a member
of $D$.

We are now ready to prove the extreme value theorem, in a version slightly more general than the one
originally given on page \pageref{extreme-value-theorem}.

The extreme value theorem: Any continuous function on a compact set achieves a maximum and minimum value,
and does so at specific points in the set.

Proof: Let $f$ be continuous, and let $C$ be the compact set on which we seek its maximum and minimum.
Then the image $D$ as defined in the lemma above is compact. Therefore $D$ contains its maximum and
minimum values.

\detour{mean-value-proof}{Proof of the mean value theorem}\index{mean value theorem!proof}

Suppose that the mean value theorem is violated. Let $L$ be the set of all $x$ in the interval from $a$ to $b$
such that $y(x)<\bar{y}$, and likewise let $M$ be the set with $y(x)>\bar{y}$. If the theorem is violated, then
the union of these two sets covers the entire interval from $a$ to $b$. Neither one can be empty; if, for example, $M$ was
empty, then we would have $y<\bar{y}$ everywhere and also $\int_a^b y=\int_a^b\bar{y}$, but it follows directly from
the definition of the definite integral that when one function is less than another, its integral is also less
than the other's. Since $y$ takes on values less than and greater than $\bar{y}$, it follows from the intermediate value theorem
that $y$ takes on the value $\bar{y}$ somewhere (intuitively, at a boundary between $L$ and $M$).

\pagebreak

\detour{fn-thm-alg-proof}{Proof of the fundamental theorem of algebra}\index{fundamental theorem of algebra!proof}

We start with the following lemma, which is intuitively obvious, because polynomials don't have asymptotes.
Its proof is given after the proof of the main theorem.

Lemma: For any polynomial $P(z)$ in the complex plane, its magnitude $|P(z)|$ achieves its minimum
value at some specific point $z_\zu{o}$.

The fundamental theorem of algebra: In the complex number system, a nonzero nth-order polynomial has exactly $n$
roots, i.e., it can be factored into the form $P(z)=(z-a_1)(z-a_2)\ldots(z-a_n)$,
where the $a_i$ are complex numbers.

Proof: The proofs in the cases of $n=0$ and 1 are trivial, so our strategy is to
reduce higher-$n$ cases to lower ones. If an nth-degree polynomial $P$ has at least
one root, $a$, then we can always reduce it to a polynomial of degree $n-1$ by
dividing it by $(z-a)$. Therefore the theorem is proved by induction provided that
we can show that every polynomial of degree greater than zero has at least one
root.

Suppose, on the contrary, that there is an nth order polynomial $P(z)$, with $n>0$, that has
no roots at all. Then by the lemma $|P|$ achieves its minimum value
at some point $z_\zu{o}$. To make things more
simple and concrete, we can construct another polynomial $Q(z)=P(z+z_\zu{o})/P(z_\zu{o})$,
so that $|Q|$ has a minimum value of 1, achieved at $Q(0)=1$. This means that
$Q$'s constant term is 1. What about its other terms? Let $Q(z)=1+c_1z+\ldots+c_nz^n$.
Suppose $c_1$ was nonzero.
Then for infinitesimally small values of $z$, the terms of order $z^2$ and higher
would be negligible, and we could make $Q(z)$ be a real number less than one by
an appropriate choice of $z$'s argument. Therefore $c_1$ must be zero. But that means
that if $c_2$ is nonzero, then for infinitesimally small $z$, the $z^2$ term dominates
the $z^3$ and higher terms, and again this would allow us to make $Q(z)$ be real and
less than one for appropriately chosen values of $z$. Continuing this process, we
find that $Q(z)$ has no terms at all beyond the constant term, i.e., $Q(z)=1$. This
contradicts the assumption that $n$ was greater than zero, so we've proved by
contradiction that there is no $P$ with the properties claimed.

Uninteresting proof of the lemma: Let $M(r)$ be the minimum value of $|P(z)|$ on the disk defined by $|z| \le r$.
We first prove that $M(r)$ can't asymptotically approach a minimum as $r$ approaches infinity.
Suppose to the contrary: for every $r$, there is some $r'>r$ with $M(r')<M(r)$.
Then by the transfer principle, the same would have to be true for
hyperreal values of $r$. But it's clear that if $r$ is infinite, the lower-order terms of $P$
will be infinitesimally small compared to the highest-order term, and therefore $M(r)$ is
infinite for infinite values of $r$, which is a contradiction, since by construction $M$ is
decreasing, and finite for finite $r$. We can therefore conclude
by the extreme value theorem that $M$ achieves its minimum for some specific value of $r$.
The least such $r$ describes a circle $|z|=r$ in the complex plane, and the minimum of $|P|$
on this circle must be the same as its global minimum. Applying the extreme value function
to $|P(z)|$ as a function of arg $z$ on the interval $0 \le \zu{arg} z \le 2\pi$, we establish
the desired result.
